[MUSIC]
Welcome to this lecture of the course on
Process Mining, Data Science and Action.
Today is the last lecture
on process discovery.
The aim is to illustrate,
that there are many more approaches
than the ones that we have seen so far.
We have seen the alpha algorithm.
We have seen the heuristic
mining algorithm.
We have seen region-based approaches.
But there are many more.
And we cannot discuss all of these
different process mining techniques, but
we will discuss a few more.
As you can see on this slide,
there are many choices that one can make.
If one looks at a process discovery
technique, one should look at things
like what is the balance between fitness,
simplicity, precision and generalization?
What is the speed?
So, how fast will the analysis
technique produce results.
How much memory is used?
What is the representation that is being
used by the discovery technique and
how flexible is the approach?
Can we also explore related problems?
One should always distinguish between the
implementation of an algorithm that can be
fast or
slow versus the approach itself, and
it will often be mixed and
a combination of two.
Let's take a step back.
So we have been talking
about process discovery and
we can look at it from the angle
of adding and removing behavior.
So a first question would be,
we see here a Petri net,
one that we have seen many times before.
How can we add behavior
to this model in terms of
edit operations on the Petri net itself?
The first approach to add behavior
is by adding transitions.
Here I added a transition x and
that allows for new traces.
For example, the trace xed,
that was not possible before.
Another approach is that we
can also simply remove places.
If we remove the place in between a and
b, we can do b before a.
So to trace b, a, c,
d becomes possible by removing this place.
We can also remove or add arcs,
just changing the behaviour.
If we remove an arc from
a place to a transition,
we allow for more behavior because
essentially, we are removing a constraint.
If we add arcs from a transition to
a place, we can also add behavior.
So that was about adding behavior,
let's now take a look at
the problem of removing behavior.
So again, we see the same Petri net.
How can we remove behavior,
no longer allowing certain traces?
The answer to this question is
the opposite of the answer to
the previous question.
So first of all,
we can remove transitions.
By removing transitions,
we remove behavior.
So if remove e,
the trace aed is no longer possible.
We can also add places.
So by adding the highlighted
place in this model, we enforce
that b will be followed by c and that
we cannot do c before b has been done.
So adding places is like
adding extra constraints.
Finally we can again remove or
add arcs and thus changing the behavior.
If we remove an arc from
a transition to a place,
what we are doing is we
are disallowing potential behavior.
We can also add a arc from places
to transitions having the same
type of effect.
So adding and
removing places is influencing
whether we have more or less behavior.
So a viewpoint that one can
take that we have used before,
is that you can view at process discovery
as the problem of finding places.
So this is a Petri net that allows for
any behavior composed of
the activities that are listed here and
adding places is like adding constraints.
So when we looked at the alpha algorithm,
we tried to discover places to
constrain the behavior to the behavior
that we had seen in the log.
When we were using state-based regions,
we first created the transition system and
then tried to group sets of states in
the transition system into places
of the corresponding process model.
Today, one of the techniques that we will
briefly discuss is language-based regions.
It's similar to state-based regions, but
we do not construct the transition system,
we start directly from traces.
So the first approach that we will
discuss today, are language based
regions and it is basically
solving a system of in-equations.
So if you look at this equation,
it looks very scary.
A and
A prime are matrices representing the log,
x and
y correspond to arcs in the Petri net, and
c corresponds to the number of tokens in
the place that we are trying to discover.
So it looks very complicated,
but it just says that
whenever we are replaying the log,
a place can never become negative.
There can be no negative number of tokens.
Any solution of this system of
in-equations is a region and
such a region corresponds to a place that
we can add without removing any behavior.
So as I said,
it looks very complicated, therefore I use
a very small example to illustrate it.
Suppose that we have a lock
where we see the traces b, a and
a, b, and we see both 30 times.
Then the language, the prefix closed
language generated by these traces is
a language consisting of the anti-trace,
the trace just consisting of a,
the trace just consisting of b,
the trace consisting of a and b, and
the trace consisting of b and a.
If we then look at these
non empty prefixes,
then we can compute how many
times a transition has fired in
each of these prefixes and that way,
we can create this matrix A.
If you look at the first row of matrix
A, there is the prefix a, so just
a has happened, and the numbers indicate
a happened once, b happened zero times.
If you look at the last row of this
matrix, if we look at the prefix b,
a, we can see that a happens once,
and b happens once.
So that's how a is defined.
In a similar way, we define a-prime,
it's again a matrix.
But now it is based on all the steps that
have been taken, except the last one.
So, we ignore the last step in the trace.
So for example, if we take a look at
the last row in this matrix a prime,
what you will see is there is the sequence
b, a, we ignore the last one,
so we see that only b
has happened one time.
And so, we have these matrices,
a and a prime, and
we can use them to create
an in-equation system.
For the small example
that I just listed here
these are all the equations that
are ensuring that at no point in time,
the number of tokens in the place that we
are trying to discover will go negative.
So what is it composed of?
We have c,
that's the initial number of tokens in
the place that we are trying to discover.
We have a and a prime, the ones that
we have seen before and we have two
column vectors that
correspond to the input and
output arcs of the place that
we are trying to discover.
So xa is the number of arcs
from transition a to the place,
xb is a number of arcs from transition b
to the place that we are trying to
discover and these are variables.
We do not know them yet.
This is exactly what we
are trying to discover.
you is the number of arcs from
the place that we are trying to
discover through transition a.
yb is the number of arcs from
the place to transition b.
Okay, so if we take a step back,
then the first part of
the system of in-equations
corresponds to the initialization.
What are the tokens that
are there at the beginning?
Then there is the part that is
just concerned with production.
It is based on a prime.
And then there is the part that
is concerned with consumption,
that's why the minus sign is there.
And here we use matrix A and
the result should always be positive.
So if we look at this in-equation
system and we rewrite it,
we find these four in-equations.
So the first row says that c should
be at least the same as ya.
So that means that the initial number of
tokens in the place should at least be
the number of arcs from
the place to transition a.
The same holds for
all the other in-equations,
you can read them in the same way.
So again, xa and b, and
xb are the arcs from
transitions to the place.
So from transition a to the place,
from transition b to the place.
ya and yb are the number of arcs from
the place to transitions a and b.
So any solution of this system
of inequations is a place.
So let's take a look at
an example solution.
What we see here are two places and
both of these places correspond to
a solution of this in-equation system.
So if we take a look at the first one,
c is equal to 1 and
ya is also equal to 1 and
this models that there is an arc
connecting the place to transition a.
For example, yb is 0 because there
is no arc from b to this place.
The other place can be
represented by this.
So c equals 1 and yb equals 1.
This indicates that there is a connection
between the place and transition b.
And if you fill in these values,
you will indeed see that all
the constraints are satisfied.
So, language-based regions
are a very powerful method and
I can only talk about
a general concept here.
Any solution of the system of in-equations
correspond to a feasible place,
a place that does not disallow
the behavior that we have seen.
It is very easy to add
additional constraints.
So for example, we want many of
the places to be empty at the end,
we can just add a constraint.
If we would like to ensure that
the place has never more than two input
transitions or two output transitions,
we can add that as a constraint.
We can also put all kinds of constraints
on the network structure just by
adding additional in-equations.
Moreover, we use a goal function because
we are not interested in all places.
We are interested in the places
that restrict the behavior most and
we may have certain preferences for
discovering a particular type of places.
So we can put that into the goal function,
and then we get a standard
optimization problem that we can use
that we can solve using ILP techniques.
So this is an example of another
more sophisticated approach than
the approaches we have been looking at so
far.
But these, and also the other approaches
that I will talk about are supported by
ProM, as you can see here.
Let's now take a look at
a completely different technique,
genetic process mining.
What is the basic idea of genetic
process mining, because again,
I'm not able to go into the details
of this particular method.
The idea is that you use
evolution to continuously improve
your model in many different
iterations ending up
with a model that describes
the log in the best way possible.
So how does it work?
We start from an event log and
from this event log,
we can randomly create process models
indicated here by the blue stars.
Every blue star corresponds
to a process model, and
we can just generate them randomly
to create the first generation.
Then we can measure the quality.
If the quality is already good enough,
which will not be the case,
then we can stop.
Often it's not, but
we look at the quality to see what
are good candidates to continue with.
The best candidates go to the next round.
We may also take multiple candidates.
I use a so called crossover operator
to recombine them and
to create new models, and
we can use mutation to change something in
the model and hope that it will improve.
But we are all doing this in a random way.
We get a new generation of process models.
In each generation,
there may be thousands of process models.
We again check the quality using
conformance checking of all of
these models.
Select again the best ones, etc, etc.
So after perhaps hundreds or
thousands of iterations, we end
up with a model that has a good quality
and that is the model that we return.
So what are the properties
of genetic mining?
It is typically extremely slow.
If we look at real life logs and
you need large generations,
many generations to get
to the desired result.
But it is extremely flexible.
It is very easy to add new forces to the
four forces that we have looked at before.
So we can besides looking at fitness,
precision, generalization, and
simplicity, we can add new forces
like what is the added distance
compared to some reference model or
we may have some cost information or
risk information and we can incorporate
this in the discovery problem
giving weights to all the dimensions
that we would like to consider.
We often use genetic process mining if
we are confronted with a new problem.
We have no ways of solving
it efficiently yet, but
we just want to experiment with it.
Based on these techniques,
you often get ideas to implement
much faster discovery techniques.
Okay, the last approach that we will
look at today is inductive mining.
So, what is the idea of inductive mining?
We take an event log and we try to
decompose it into different parts.
So every row that you see here corresponds
to a trace and we would like to split or
decompose these traces in a different way,
so
that we can see the underlying
structure of the process.
So what we are going to do first,
is we are going to partition
the set of labels into two sets.
So the set of all labels is partitioned
into a set a,b,c,d and a set e,f,g.
If we do that and we project the log on
these two sets, then this is what we get and
so the first trace,
abdef is decomposed in abd and ef.
And if we use the sequenced operator and
we glue these things together,
we get again the original traces.
Of course, the question is how did
I decide here what operator to use?
That is the basic idea of the technique
how you can discover these
types of things.
But here I'm just
explaining the main ideas.
So after using the sequence operator, we
can look at the first part of the log and
we again see that everything
starts with an a.
So again we can split there
using the sequence operator.
And for example if we now look at
the first row, we have a which
is sequentially followed by bd,
which is sequentially followed by e and f.
So we are breaking down
the log in smaller parts.
Now we can take a look at the last
column where we see ef and eg.
So again it's logical to,
to split this using the sequence
operator and this is the result.
Now there is nothing that we can split
based on the sequence operator anymore,
but we can try other operators.
So if we take a look at the last column,
it contains the symbols,
the activities f and g and we can split
it based on f and g, but not
in the sense of a sequence,
in the sense of an XOR split.
So we can split it like this.
So what now?
With the XOR operator,
you can see that there is either an f or
a g and the f and
gs are in different columns, but
there is never a situation that there is
a symbol in both columns at the same time.
So, that is the XOR operator.
If we take a look at
the second column,
we can see that we cannot use sequence.
It is very natural to see to apply
here the so-called parallel operator.
So we split the set bcd and
the sets bc and d using AND decomposition.
If we do that, this is what we get.
We're still not done because if you
look at the second column which is
part of this parallel operator,
you can see that there are still two
different types of activities, b and c.
So now we can again apply the XOR
operator, split it into the sets
consisting just b and the set consisting
of just c and this is what you get.
So if I now look again at
what we have been doing,
we have been splitting the log until no
further decomposition is possible, and
you can represent that as a tree.
So a tree as is shown here,
is called a process tree.
We decompose the log based on AND,
XOR, sequences,
but also operators like loops
that you cannot see here.
If you look at this process
tree we can easily convert it
to a structured process model, for example
here, you see the corresponding Petri net,
and here you can see
the corresponding BPMN model.
What you can see is that these models
can indeed replay all the traces that we
have seen before.
So, they are a natural representation
of the behavior that we have seen.
So, that's the idea of the inductive miner
and it is supported, by the ProM tool set.
So here you see the tool in action,
you can also see that different
cases are being animated,
just as we have seen in the fuzzy miner
and we have seen in the disco tool.
The underlying representation is a process
tree, that is what is shown here, but
such a process tree can be automatically
converted into a Petri net or
a BPMN diagram.
So we can choose whatever
notation we like.
This inductive miner also has a slider and
you can choose the level of
abstraction that you can look at.
So here you can see that this is
based on the same event log, but
I now selected fewer activities.
I just look at the most frequent
activities and in this way,
I get the simpler model.
This mining tool has many more features.
It can also show deviations
you can look at frequent and
infrequent paths and so
it's really a tool where you
can explore a data set within
a very comprehensive manner.
So these are examples of more advanced,
more sophisticated process discovery
techniques than we have seen before.
There are many more but
this was everything there was in
this course about process discovery.
In the next lecture, we will start talking
about conformance checking which is
also a very important process mining task.
Thank you for watching this lecture.
See you next time.
[MUSIC]

