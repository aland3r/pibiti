1
00:00:00,000 --> 00:00:07,274
[MUSIC]

2
00:00:07,274 --> 00:00:08,920
Welcome to this lecture.

3
00:00:08,920 --> 00:00:12,250
In the last lecture, we showed that
the combination of process mining and

4
00:00:12,250 --> 00:00:16,200
data mining can be used to learn
more about decision points.

5
00:00:16,200 --> 00:00:20,110
The information can be used to
enrich the process model, and

6
00:00:20,110 --> 00:00:23,750
serve as a starting point for
all kinds of process improvements.

7
00:00:25,780 --> 00:00:30,820
So the focus of the last lecture
was on mining decision points.

8
00:00:30,820 --> 00:00:33,320
We have one response variable.

9
00:00:33,320 --> 00:00:38,180
Namely the activity that is being chosen
and we would like to understand this

10
00:00:38,180 --> 00:00:43,370
response variable in terms of predictor
variables, like which was the person that

11
00:00:43,370 --> 00:00:49,810
executed the last activity or what is the
type of customer that is being processed?

12
00:00:49,810 --> 00:00:52,880
So this is just a starting point for
all kinds of

13
00:00:52,880 --> 00:00:59,060
analysis where we combine data mining
together with process mining.

14
00:00:59,060 --> 00:01:03,580
So today's focus is on
discovering the guards.

15
00:01:03,580 --> 00:01:06,690
So taking the output
of a decision tree and

16
00:01:06,690 --> 00:01:12,570
translating it to a data-aware Petri net,
so a process model extended with data.

17
00:01:14,690 --> 00:01:18,820
So very concretely, if we take a look
at this example where there is a choice

18
00:01:18,820 --> 00:01:23,800
between b and c, we would like
to create guards conditions for

19
00:01:23,800 --> 00:01:27,270
b and c describing which cases,

20
00:01:27,270 --> 00:01:31,270
under which circumstances we do b,
and under which circumstances we do c.

21
00:01:33,040 --> 00:01:34,610
So how does it work?

22
00:01:34,610 --> 00:01:40,077
If we focus on the choice between b and
c, following a,

23
00:01:40,077 --> 00:01:45,730
we can take as response variable,
whether b or c is done.

24
00:01:45,730 --> 00:01:49,420
And as predictor variables,
we can, for example,

25
00:01:49,420 --> 00:01:54,960
look at the attributes of the event
corresponding to activity a.

26
00:01:54,960 --> 00:01:58,370
We could also take other predictor
variables like the weather or

27
00:01:58,370 --> 00:01:59,900
how busy things are.

28
00:01:59,900 --> 00:02:06,310
But here we focus on the attributes
of the preceding event.

29
00:02:06,310 --> 00:02:09,960
So here you see a table with data

30
00:02:09,960 --> 00:02:15,336
describing events executing for
different cases.

31
00:02:15,336 --> 00:02:22,238
So if we look at this table, we only
have to focus on activities a, b, and c.

32
00:02:22,238 --> 00:02:24,327
We only have to look at

33
00:02:24,327 --> 00:02:27,700
whether b of c is being executed.

34
00:02:27,700 --> 00:02:31,390
We do not need to look at
the attributes associated to it.

35
00:02:31,390 --> 00:02:36,060
With respect to a,
we need to look at the variables,

36
00:02:36,060 --> 00:02:39,910
the attributes corresponding
to this a event.

37
00:02:39,910 --> 00:02:43,930
We can combine such
information as is shown here.

38
00:02:43,930 --> 00:02:49,380
So, for example for
case 1 John executed activity a, and

39
00:02:49,380 --> 00:02:51,570
then this was followed by activity b.

40
00:02:52,640 --> 00:02:56,410
So we can take this information and
convert it into

41
00:02:56,410 --> 00:03:01,730
a new table that we can use
for the decision tree learning.

42
00:03:02,990 --> 00:03:08,440
So here, if we take a look at the first
row, it corresponds to the choice for

43
00:03:08,440 --> 00:03:14,710
activity b, which was following
activity a, which was executed by John,

44
00:03:14,710 --> 00:03:19,310
done for a silver customer and
corresponding to an amount of 500 Euros.

45
00:03:20,870 --> 00:03:26,180
So this is how we create
a decision tree learning problem.

46
00:03:27,910 --> 00:03:32,030
If we do this, then using
the techniques that we saw in week 1,

47
00:03:32,030 --> 00:03:34,620
we can create a decision tree.

48
00:03:34,620 --> 00:03:39,470
So, for example, here we decided to
split on the attribute customer.

49
00:03:39,470 --> 00:03:44,930
And we get a decision tree which
is classifying instances based

50
00:03:44,930 --> 00:03:49,480
on whether they are silver customers or
gold customers.

51
00:03:49,480 --> 00:03:53,980
And it turns out that silver and
gold customers typically have

52
00:03:53,980 --> 00:03:56,800
different characteristics
when it comes to this choice.

53
00:03:58,600 --> 00:04:03,570
We can encode the information that we see
in the decision tree in the process model.

54
00:04:03,570 --> 00:04:07,800
And the process model now very clearly
shows that for silver customers, we do b.

55
00:04:08,810 --> 00:04:11,090
For gold customers, we execute c.

56
00:04:13,130 --> 00:04:17,620
So the result of doing this, and
if we do that for every choice,

57
00:04:17,620 --> 00:04:23,020
is a data-
 process model, where
we combine control flow with data flow.

58
00:04:25,550 --> 00:04:28,300
So let's take a look
at another example and

59
00:04:28,300 --> 00:04:31,180
see whether you understand these things.

60
00:04:31,180 --> 00:04:36,320
So look at the second decision
point in the same process.

61
00:04:36,320 --> 00:04:41,370
Take a look at the decision tree
which was discovered based on

62
00:04:41,370 --> 00:04:43,009
a set of predicted valuables.

63
00:04:44,110 --> 00:04:49,830
And convert this decision tree into
guards for the second decision point.

64
00:04:51,480 --> 00:04:52,950
This slide shows the answer.

65
00:04:54,340 --> 00:04:58,760
So what you see is that we add
conditions to all the different

66
00:05:00,060 --> 00:05:05,190
transitions involved in
this particular choice.

67
00:05:05,190 --> 00:05:09,780
And so we can see that there
are three classes, pay, reject, or

68
00:05:09,780 --> 00:05:11,670
reinitiate the request.

69
00:05:12,800 --> 00:05:17,050
And these correspond to different
leaves of this decision tree.

70
00:05:17,050 --> 00:05:19,660
And if you follow the paths
in the decision tree and

71
00:05:19,660 --> 00:05:23,900
convert them to logical expressions,
you will see that this is the case.

72
00:05:23,900 --> 00:05:31,310
Note that there is at any point in time,
just one of the guards evaluates to true.

73
00:05:31,310 --> 00:05:34,500
It cannot be that multiple
guards evaluate to true.

74
00:05:36,020 --> 00:05:40,050
However if we look at the decision tree,
we can see that

75
00:05:40,050 --> 00:05:44,549
there are certain leaves where we
are more certain about than other leaves.

76
00:05:45,710 --> 00:05:48,230
So the
leaves that are indicated in red,

77
00:05:50,710 --> 00:05:55,820
they are classified
according to a certain class.

78
00:05:55,820 --> 00:06:02,600
But if you look at the underlying data
there is not consensus about these things.

79
00:06:02,600 --> 00:06:05,490
And not enough information
gain could be achieved.

80
00:06:06,840 --> 00:06:12,590
If we have such uncertainty, we can
also add that uncertainty to the model,

81
00:06:12,590 --> 00:06:16,470
by making non-deterministic decisions.

82
00:06:16,470 --> 00:06:21,670
So decisions where multiple transitions
are enabled and we can pick one of them.

83
00:06:22,940 --> 00:06:25,310
So if we take a look at
this particular example.

84
00:06:26,690 --> 00:06:34,470
All the leaves that correspond to red,
lead to the situation where all tree

85
00:06:34,470 --> 00:06:39,330
transitions are enabled at the same
time but only one of them is chosen.

86
00:06:40,560 --> 00:06:45,436
And so if you look at the guards,
you will see that they are now weaker and

87
00:06:45,436 --> 00:06:47,808
therefore they are overlapping.

88
00:06:47,808 --> 00:06:54,849
It can be the case that for some
cases, all tree guards evaluate to true.

89
00:06:58,119 --> 00:07:04,125
We look at the decision tree and
the data underneath the decision tree,

90
00:07:04,125 --> 00:07:10,640
we can also see that there is,
often no consensus.

91
00:07:10,640 --> 00:07:16,400
And there are certain probabilities
associated to the leaves that are shown.

92
00:07:17,740 --> 00:07:21,580
So we can encode also
this into the process model.

93
00:07:21,580 --> 00:07:26,220
So then we get a process model that
depending on data attributes, assigns

94
00:07:26,220 --> 00:07:31,740
certain probabilities to transitions,
and that is illustrated in this diagram.

95
00:07:33,010 --> 00:07:35,540
It's a bit dense so if we zoom in and

96
00:07:35,540 --> 00:07:41,740
we look at the guards corresponding
to the choice to pay the request.

97
00:07:42,920 --> 00:07:48,020
Then you can see that the guard is
following the decision tree and

98
00:07:48,020 --> 00:07:50,850
then looking at
the different probabilities.

99
00:07:50,850 --> 00:07:53,280
So for example,
if examination is not okay,

100
00:07:53,280 --> 00:07:58,520
then we can look at the decision
tree at the underlying data, and

101
00:07:58,520 --> 00:08:04,315
then we can see that in one of the 30
cases, where examination was no okay.

102
00:08:05,600 --> 00:08:08,280
Actually a payment took place, and

103
00:08:08,280 --> 00:08:12,320
we can add this as a probability
to the process model.

104
00:08:12,320 --> 00:08:17,120
We can see a combination of
probabilistic elements and

105
00:08:17,120 --> 00:08:20,840
of using the data of the cases
associated to them.

106
00:08:23,310 --> 00:08:26,650
The models that we learn
may sometimes look

107
00:08:26,650 --> 00:08:30,600
a bit strange with very strange guards.

108
00:08:30,600 --> 00:08:33,720
And therefore it's very important to
understand that the models that we

109
00:08:33,720 --> 00:08:36,010
are discovering are descriptive.

110
00:08:36,010 --> 00:08:38,460
They are describing what happened.

111
00:08:38,460 --> 00:08:41,700
They're not describing what should happen.

112
00:08:41,700 --> 00:08:45,150
If you have models that are describing
what should have happened,

113
00:08:45,150 --> 00:08:47,310
we are looking at prescriptive models.

114
00:08:49,570 --> 00:08:53,180
This is very important
to see this difference.

115
00:08:53,180 --> 00:08:57,370
Both descriptive and
prescriptive models can be used for

116
00:08:57,370 --> 00:08:59,439
conformance checking as
we have seen before.

117
00:09:00,640 --> 00:09:04,380
And everything that was discussed in
the context of conformance checking can

118
00:09:04,380 --> 00:09:07,490
also be applied to Petri net's bit data.

119
00:09:07,490 --> 00:09:10,830
And so
if you take a look at this Petri net,

120
00:09:10,830 --> 00:09:14,430
with these guards,
we can replay the event log and

121
00:09:14,430 --> 00:09:19,980
we can detect where there are deviations
between reality and the model.

122
00:09:19,980 --> 00:09:26,050
All of the things that we have discussed
so far are also supported by ProM.

123
00:09:26,050 --> 00:09:30,880
For example here you see a screen shot
of ProM where we take an event log and

124
00:09:30,880 --> 00:09:36,240
we take a control flow model,
not holding any information about data and

125
00:09:36,240 --> 00:09:41,010
we combine them to discover
a data-aware Petri net.

126
00:09:41,010 --> 00:09:45,070
So this is the result of applying
the plug-in on this event log and

127
00:09:45,070 --> 00:09:47,250
this classical Petri net model.

128
00:09:47,250 --> 00:09:50,900
And we get a Petri net extended with time.

129
00:09:50,900 --> 00:09:53,900
So the two decision points
are highlighted here.

130
00:09:53,900 --> 00:09:57,100
And if we zoom in,
we can see that there are two

131
00:09:57,100 --> 00:10:02,150
variables that are influencing
the decisions that we see here.

132
00:10:02,150 --> 00:10:09,360
The dashed lines correspond to writes and
reads, and the transitions

133
00:10:09,360 --> 00:10:15,360
that are highlighted in red or have a red
name, they have guards associated to them.

134
00:10:15,360 --> 00:10:17,120
So let's take a look at these guards.

135
00:10:18,660 --> 00:10:24,010
If you look at the top guard,
you can see that we invite reviewers,

136
00:10:24,010 --> 00:10:28,400
additional reviewers, if there are two or

137
00:10:28,400 --> 00:10:32,970
less accepts or
there are two or less rejects.

138
00:10:34,990 --> 00:10:39,030
If there are three or
more accepts or there are three or

139
00:10:39,030 --> 00:10:42,970
more rejects,
we make the decision to accept or reject.

140
00:10:42,970 --> 00:10:45,400
That is reflected by these guards.

141
00:10:45,400 --> 00:10:49,220
We can also the second decision
point where the actual decision is

142
00:10:49,220 --> 00:10:51,950
made to accept or reject.

143
00:10:51,950 --> 00:10:54,350
We take one of the two paths and
then go to the end.

144
00:10:56,330 --> 00:11:00,730
We can also use conformance
checking in conjunction with

145
00:11:00,730 --> 00:11:02,610
these data-aware Petri nets.

146
00:11:02,610 --> 00:11:06,310
So we take an event log and
a data-aware Petri net, and

147
00:11:06,310 --> 00:11:10,970
we can do conformance checking and we can
show where the deviations take place.

148
00:11:10,970 --> 00:11:16,240
And so here you can see
data-aware alignments also taking

149
00:11:16,240 --> 00:11:19,570
into account possible
problems related to data.

150
00:11:21,770 --> 00:11:26,560
So, in this lecture,
we have seen that we can combine data and

151
00:11:26,560 --> 00:11:28,910
processes in a very natural way.

152
00:11:29,910 --> 00:11:36,060
In that way, we can also clearly see that
there is a very nice interaction and

153
00:11:36,060 --> 00:11:40,240
integration possible between data
mining and process mining, and

154
00:11:40,240 --> 00:11:43,420
in the next couple of lectures,
we will see more examples of that.

155
00:11:44,970 --> 00:11:49,090
If you would like to read more
about discovering guards and

156
00:11:49,090 --> 00:11:54,120
the combination of data mining and
process mining, please read chapter 8.

157
00:11:54,120 --> 00:11:57,985
Thank you for watching,
and hope to see you soon.

158
00:11:57,985 --> 00:12:07,985
[MUSIC]

